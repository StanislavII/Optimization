{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d57e390",
   "metadata": {},
   "source": [
    "# Линейные системы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d84d8aa",
   "metadata": {},
   "source": [
    "В лекции про $LU$ разложение мы обуславливались, что в системе $Ax = b$ работаем с квадратными матрицами $A \\in R^{n\\times n}$ и также говорили, что такие системы называются определенными. Но нам также надо считать решение и для других систем, в этом блоке будут разобраны **переопределенные системы**, когда $A \\in R^{m\\times n}, m > n$. Пример такой системы $A \\in R^{3\\times 2}$ (3 наблюдения, 2 атрибута):\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "3 & -6 \\\\\n",
    "4 & -8 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "7 \\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Мы разумеется могли бы сделать $LU$ разложение для данной матрицы:\n",
    "\n",
    "$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "3 & -6 \\\\\n",
    "4 & -8 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}, b = \n",
    "\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "7 \\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "& PA = LU \\\\\n",
    "&\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "3 & -6 \\\\\n",
    "4 & -8 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "\\frac{4}{3} & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "3 & -6 \\\\\n",
    "0 & 1 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix} \\\\\n",
    "&\\tilde b = \n",
    "\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "2 \\\\\n",
    "7 \n",
    "\\end{bmatrix} \\\\\n",
    "& Ly = \\tilde b, \\quad \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "\\frac{4}{3} & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "2 \\\\\n",
    "7 \n",
    "\\end{bmatrix}, \\quad \\tilde y = (-1, 2, \\frac{25}{3}) \\\\\n",
    "& Ux = \\tilde y, \\quad \\begin{bmatrix}\n",
    "3 & -6 \\\\\n",
    "0 & 1 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "2 \\\\\n",
    "\\frac{25}{3}\n",
    "\\end{bmatrix}, \\quad \\tilde x = (\\frac{11}{3}, 2)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Но увы, данное решение нам не подходить, так как в данном случае оно удовлетворяет только 2 наблюдениям из 3, а последнее игнорируется, это можно было заметить в самом начале разложение, где матрица $U$ имела нулевую строку, в общем, любое уравнение вида $A \\in R^{m\\times n}, m > n$ будет иметь $(m-n)$ нулевых строк, а нулевые строчки никак не адаптируют полученное решение из ненулевых строк. То есть решение системы $Ax =b$, полученное с помощью $LU$ алгоритма инвариантно к $(m-n)$ наблюдением и решает задачу только для $n$ ведущих миноров."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193e8252",
   "metadata": {},
   "source": [
    "## Метод наименьших квадратов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802320d",
   "metadata": {},
   "source": [
    "Предположим, что у нас есть матрица $A \\in R^{m\\times n}, m > n$, как мы только что показали, в общем случае мы не можем решить такую систему, и единственное, что мы можем сделать - это минимизировать ошибку нашего решения. Запишем задачу:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{argmin}_{x} ||Ax - b||^2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Поиск такого вектора $x$, удовлетворяющего задаче минимизации ошибки выше и называется **методом наименьших квадратов**. Решение такой задачи методом градиентного спуска - тема будующих лекций, но на данном этапе мы рассмотрим следующий способ решения.\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "&F(x) = ||Ax - b||^2 = (Ax - b)^{\\top}(Ax - b) = x^{\\top}A^{\\top}Ax - 2x^{\\top}A^{\\top}b + b^{\\top}b \\\\\n",
    "&\\nabla F(x) = 2(A^{\\top}Ax - A^{\\top}b) = 0 \\\\\n",
    "&A^{\\top}Ax = A^{\\top}b, \\quad A^{\\top}(Ax - b) = 0\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Последнее уравнение называется **нормальным уравнением**, потому что данное уравнение задает соотношение ортогогнальности между вектором ошибок $r = Ax - b$ и любым вектором $Ay$ из пространства $A$. Нам важно это соотношение тем, что остаток (ошибка) полученная методом наименьштх квадратов не зависит от направлений пространства матрицы $A$, то есть наша ошибка не смещена по какому-либо направлению и не может быть воспроизведена любой линейной комбинацией пространства $A$.\n",
    "\n",
    "#### Решение через ортогональные преобразования\n",
    "\n",
    "Напомню, что класс ортогональных матриц, если вы поворачиваете или отображаете вектор, не меняет длину вектора (его норму), например $||Qy||^2 = (Qy)^{\\top}(Qy) = y^{\\top}Q^{\\top}Qy = ||y||^2$\n",
    "\n",
    "Следовательно, если мы сделаем следующее преобразование, то логика поиска $x$ не изменится: $\\text{argmin}_{x} ||Q^{\\top}(Ax - b)||^2 = \\text{argmin}_x||Q^{\\top}Ax - Q^{\\top}b||^2$, и наш план состоит в том, чтобы применить к A последовательность ортогональных преобразований $Q$, которые приведут еe к верхнетреугольной форме. Это создаст эквивалентную, легко решаемую проблему. Для нашего примера выше:\n",
    "\n",
    "$\n",
    "\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\text{argmin}_{x\\in\\mathbb{R}^2} \\||Q^{\\top}Ax - Q^{\\top}b||^2 = \\text{argmin}_{x\\in\\mathbb{R}^2}\n",
    "\\norm{\n",
    "\\begin{bmatrix}\n",
    "r_{11} & r_{12} \\\\\n",
    "0 & r_{22} \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix} -\n",
    "\\begin{bmatrix}\n",
    "\\tilde b_1 \\\\\n",
    "\\tilde b_2 \\\\\n",
    "\\tilde b_3\n",
    "\\end{bmatrix}\n",
    "}^2\n",
    "$\n",
    "\n",
    "Где $\\tilde b = Q^{\\top}b$, заметим, что неважно какие $x_1, x_2$ мы подберем, сумма квадратов ошибок будет как минимум $\\tilde b_3^2$, поэтому мы можем вычеркнуть последнюю строчку и решать следующую систему как обычно:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "r_{11} & r_{12} \\\\\n",
    "0 & r_{22}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\tilde b_1 \\\\\n",
    "\\tilde b_2 \n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Таким образром, нам надо свести матрицу $A$ верхнетреугольной форме, но задача легче от этого не становится. Нам все еще надо решить следующее - $\\text{argmin}_{x\\in\\mathbb{R}^2} \\||Q^{\\top}Ax - Q^{\\top}b||^2$, то есть подобрать какую-то последовательность ортогональных преобразований $Q$, и было бы гораздо легче, если бы существовало следующее разложение $A = QR$, тогда бы наша задача превратилась бы в:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "& \\text{argmin}_{x\\in\\mathbb{R}^2} \\||Q^{\\top}Ax - Q^{\\top}b||^2, \\, A = QR \\\\\n",
    "& \\text{argmin}_{x\\in\\mathbb{R}^2} \\||Q^{\\top}QRx - Q^{\\top}b||^2 \\\\\n",
    "& \\text{argmin}_{x\\in\\mathbb{R}^2} \\||Rx - Q^{\\top}b||^2 \\\\\n",
    "& \\text{argmin}_{x\\in\\mathbb{R}^2} \\||Rx - \\tilde b||^2, \\, Q^{\\top}b = \\tilde b \\\\\n",
    "& Rx = \\tilde b\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Оказывается такое разложение существует и называется $\\mathbf{QR}\\textbf{-разложение!}$, но о нем немного позже.\n",
    "\n",
    "#### Решение через нормальное уравнение\n",
    "\n",
    "Также можно попробовать решить данную задачу в текущем сетапе без подбора ортогонального преобразования из полученного ранее **нормального уравнения**.\n",
    "\n",
    "Для заданной матрицы $A \\in \\mathbb{R}^{m\\times n}$ с рангом $rank(A) = n$ и $b \\in \\mathbb{R}^{n}$, мы найдем такой вектор $x_{ls}$, удовлетворяющий нормальному уравнению. \n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "& A^{\\top}Ax = A^{\\top}b \\\\\n",
    "& \\text{Посчитать} \\, C = A^{\\top}A \\\\\n",
    "& \\text{Посчитать} \\, d = A^{\\top}b \\\\\n",
    "& \\text{Разложение Холецкого:} \\, C = GG^{\\top} \\\\\n",
    "& \\text{Решить} \\, Gy =d, \\, G^{\\top}x_{ls} = y \n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba65b1",
   "metadata": {},
   "source": [
    "## QR разложение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5430e536",
   "metadata": {},
   "source": [
    "**Теорема (QR разложение). Любая $\\mathbf{A \\in \\mathbb{R}^{m \\times n}}$ может быть разложена на произведение ортогональной матрицы $\\mathbf{Q \\in \\mathbb{R}^{m \\times m}}$ и верхнетреугольной матрицы $\\mathbf{R \\in \\mathbb{R}^{m \\times n}}$.**\n",
    "\n",
    "**Теорема (Узкое QR разложение). Любая $\\mathbf{A \\in \\mathbb{R}^{m \\times n}}$ имеющая полный ранг по колонкам тогда узкое QR разложение $\\mathbf{A = Q_1R_1}$, где $\\mathbf{Q_1 \\in \\mathbb{R}^{m \\times n}}$ уникальная матрица с ортонормальными колоноками, и верхнетреугольная матрица $\\mathbf{R_1 \\in \\mathbb{R}^{n \\times n}}$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6282c",
   "metadata": {},
   "source": [
    "### QR разложение через процедуру Грамма-Шмидта"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc47116",
   "metadata": {},
   "source": [
    "#### Внешнее произведение (outer product)\n",
    "\n",
    "Тут следует немного углубиться в детали внешнего произведения и его геометрических интепретаций.\n",
    "\n",
    "Начнем с более подобного рассмотрения этого разложения. $QR$ разложение можно представить как сумму внешних произведений (outer product) колонок матрицы $Q$ и строчек матрицы $R$. Само внешнее произведение $ww^{\\top}$ определяет проекцию по направлению вектора $w$. Проекция вектора на линию (или, в более общем смысле, подпространство) — это ближайшая точка на этой линии к исходному вектору. Если $w$ — вектор, определяющий направление линии, проекция любого вектора y на линию по w можно описать матрицей $ww^{\\top}$. Разумеется матрица $ww^{\\top}$ обладает всеми свойствами проекций. \n",
    "\n",
    "Если $w$ - единичный вектор, то матрица $P = ww^{\\top}$ задает матрицу проекции на линюю, задаваемую вектором $w$. $Px = w(w^{\\top}x)$, где $w^{\\top}x$ - скалярное произведение, которое задает скалярную проекцию векторф $x$ на вектор $w$, то есть длину вектора $x$, лежащую по направлению вектора $w$, далее умножая на $w$ получаем векторную проекцию вектора $x$ по направлению вектора $w$. Таким образом внешнее произведение $ww^{\\top}$ берет любой вектор $x$ и возращает ту часть, которая лежит по напралению $w$, эффективно исключая компоненты, ортогональные данному направлению, например мы хотем найти проекцию вектора $b = (5,3)$ на вектор $a = (1,2)$, посчитаем внешнее произведение:\n",
    "\n",
    "$\n",
    "aa^{\\top} = \n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "1 & 2\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 4\n",
    "\\end{pmatrix}\n",
    "$\n",
    "и далее посчитаем матрично-векторное произведение \n",
    "$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 4\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "5 \\\\\n",
    "3\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "11 \\\\\n",
    "22\n",
    "\\end{pmatrix} = 11 \\begin{pmatrix}\n",
    "1 \\\\\n",
    "2\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "В данном примере, мы не задавали ограничение на единичность вектора, на который собирались делать отображение. И математически, мы посчитали следующее: $a^{\\top}b = ||a||||b||\\cos\\theta$ - угол между векторами не нормированный на длины векторов, тогда проекцией является длина катета треугольника на стороне вектора $a$ (расстояние от начала векторов до перпендикуляра) и это $proj_a(b) = ||b||\\cos\\theta = \\frac{a^{\\top}b}{||a||}$, а чтобы расчитать векторную составляющую компоненты $b$ по вектору $a$ надо домножить длину катета на нормированный вектор $\\frac{a}{||a||}$, и того $\\overset{\\rightharpoonup}{\\text{proj}}_a(b) = \\frac{aa^{\\top}}{a^{\\top}a}b$\n",
    "\n",
    "#### Продолжение процедуры Грамма Шмидта\n",
    "\n",
    "Произведение $A = QR$ можно рассмотреть как сумму внешних произведений колонок матрицы $Q$ и строчек матрицы $R$\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "A &= \\begin{bmatrix} | & | & & | \\\\ q_1 & q_2 & \\cdots & q_n \\\\  | & | & & |  \\end{bmatrix}  \\begin{bmatrix} - &  \\tilde{r}_1^T & - \\\\ - & \\tilde{r}_2^T & - \\\\ &  \\vdots & \\\\ - & \\tilde{r}_n^T & - \\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix} | \\\\ q_1 \\\\ | \\end{bmatrix} \\begin{bmatrix} - &  \\tilde{r}_1^T  & - \\end{bmatrix}\n",
    "+ \n",
    "\\begin{bmatrix} | \\\\ q_2 \\\\ | \\end{bmatrix} \\begin{bmatrix} - &  \\tilde{r}_2^T  & - \\end{bmatrix}\n",
    "+ \\cdots + \\begin{bmatrix} | \\\\ q_n \\\\ | \\end{bmatrix} \\begin{bmatrix} - &  \\tilde{r}_n^T  & - \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Например:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2} \\\\\n",
    "\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sqrt{2} & \\sqrt{2} \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\sqrt{2}}{2} \\\\\n",
    "\\frac{\\sqrt{2}}{2} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sqrt{2} & \\sqrt{2}\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "-\\frac{\\sqrt{2}}{2} \\\\\n",
    "\\frac{\\sqrt{2}}{2} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0 & 0\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Однако оказывается, что каждый из этих внешних произведений имеет совершенно особую структуру, то есть каждый из них имеет больше столбцов со всеми нулями. Это связано с тем, что строки $R$ имеют большое количество нулевых элементов, поскольку матрица верхнетреугольная. Рассмотрим небольшой пример для $m=5,n=3$:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\underbrace{A}_{5 \\times 3} &= \\underbrace{Q_1}_{5 \\times 3} \\underbrace{R}_{3 \\times 3} \\\\\n",
    " A &= \\begin{bmatrix} a_1 & a_2 & a_3 \\end{bmatrix} \\\\\n",
    " A  &=  \\begin{bmatrix} q_1 & q_2 & q_3 \\end{bmatrix}  \\begin{bmatrix} r_1^T \\\\ r_2^T \\\\ r_3^T \\end{bmatrix} = \\begin{bmatrix} q_1 & q_2 & q_3 \\end{bmatrix} \\begin{bmatrix} \\times & \\times & \\times \\\\ 0 & \\times & \\times \\\\ 0 & 0 & \\times \\end{bmatrix} =  q_1r_1^T + q_2 r_2^T + q_3 r_3^T \\\\\n",
    "      A &= \\begin{bmatrix}\\times & \\times & \\times \\\\ \\times & \\times & \\times \\\\ \\times & \\times & \\times\\\\ \\times & \\times & \\times \\\\ \\times & \\times & \\times \\end{bmatrix} +\n",
    "      \\begin{bmatrix} 0 & \\times & \\times \\\\ 0 & \\times & \\times  \\\\ 0 & \\times & \\times  \\\\ 0 & \\times & \\times  \\\\ 0 & \\times & \\times  \\end{bmatrix} +\n",
    "      \\begin{bmatrix}  0 & 0 & \\times \\\\  0 & 0 & \\times \\\\  0 & 0 & \\times \\\\  0 & 0 & \\times \\\\  0 & 0 & \\times \\end{bmatrix}  \\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Где $\\times$ обозначает потенциально ненулевой элемент матрицы. Рассмотрим очень интересный факт: если выполнена приведенная выше эквивалентность, то вычитая полную матрицу $q_1r_1^{\\top}$ мы гарантированно получим матрицу хотя бы с одним нулевым столбцом. Назовем вложенную матрицу $A^{(2)}$:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "    \\begin{bmatrix} | & \\\\ 0 & A^{(2)} \\\\ | & \\end{bmatrix} &= A - \\begin{bmatrix}\\times & \\times & \\times \\\\ \\times & \\times & \\times \\\\ \\times & \\times & \\times\\\\ \\times & \\times & \\times \\\\ \\times & \\times & \\times \\end{bmatrix} = \\begin{bmatrix} 0 & \\times & \\times \\\\ 0 & \\times & \\times  \\\\ 0 & \\times & \\times  \\\\ 0 & \\times & \\times  \\\\ 0 & \\times & \\times  \\end{bmatrix} +\n",
    "      \\begin{bmatrix}  0 & 0 & \\times \\\\  0 & 0 & \\times \\\\  0 & 0 & \\times \\\\  0 & 0 & \\times \\\\  0 & 0 & \\times \\end{bmatrix} \\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Мы можем обощить процесс получения $A^{(k)}$, в котором будем вычислять столбец матрицы $Q$, который назовем $q_k$:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "      &A = \\sum\\limits_{i=1}^n q_r r_i^T = \\sum\\limits_{i=1}^{k-1} q_i r_i^T + \\sum\\limits_{i=k}^n q_i r_i^T \\\\\n",
    "      &A - \\sum\\limits_{i=1}^{k-1} q_i r_i^T = \\sum\\limits_{i=k}^n q_i r_i^T \\\\\n",
    "      &\\begin{bmatrix} 0 & A^{(k)} \\end{bmatrix} e_k = \\sum\\limits_{i=k}^n q_i r_i^T e_k = q_k r_k^T e_k + q_{k+1} r_{k+1}^T e_k + \\cdots + q_n r_n^T e_k = q_k r_{kk} \n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Домнажая на $e_k = (0, \\underbrace{\\dots}_{k-3}, 0, 1, \\dots, 0)$ мы сравниваем $k$-ые колонки с каждой стороны. Проверим, действительно ли это так, возьмем разложение:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "&\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "1 & 1 & 0\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\sqrt{3}}{3} & -2\\frac{\\sqrt{15}}{15} & 3\\frac{\\sqrt{35}}{35} \\\\\n",
    "\\frac{\\sqrt{3}}{3} & \\frac{\\sqrt{15}}{15} & \\frac{\\sqrt{35}}{35} \\\\\n",
    "0 & \\frac{\\sqrt{15}}{5} & 3\\frac{\\sqrt{35}}{35} \\\\\n",
    "\\frac{\\sqrt{3}}{3} & \\frac{\\sqrt{15}}{15} & -4\\frac{\\sqrt{35}}{35}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sqrt{3} & \\frac{2}{3}\\sqrt{3} & \\frac{2}{3}\\sqrt{3} \\\\\n",
    "0 & \\frac{\\sqrt{15}}{3} & 2\\frac{\\sqrt{15}}{15} \\\\\n",
    "0 & 0 & \\frac{\\sqrt{35}}{5}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & \\frac{2}{3} & \\frac{2}{3} \\\\\n",
    "1 & \\frac{2}{3} & \\frac{2}{3} \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "1 & \\frac{2}{3} & \\frac{2}{3}\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0 & -\\frac{2}{3} & -\\frac{4}{15} \\\\\n",
    "0 & \\frac{1}{3} & \\frac{2}{15} \\\\\n",
    "0 & 1 & \\frac{2}{5} \\\\\n",
    "0 & \\frac{1}{3} & \\frac{2}{15}\n",
    "\\end{bmatrix} + \\\\\n",
    "&\\begin{bmatrix}\n",
    "0 & 0 & \\frac{3}{5} \\\\\n",
    "0 & 0 & \\frac{1}{5} \\\\\n",
    "0 & 0 & \\frac{3}{5} \\\\\n",
    "0 & 0 & -\\frac{4}{5}\n",
    "\\end{bmatrix} \\\\\n",
    "&\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "1 & 1 & 0\n",
    "\\end{bmatrix} - \\begin{bmatrix}\n",
    "1 & \\frac{2}{3} & \\frac{2}{3} \\\\\n",
    "1 & \\frac{2}{3} & \\frac{2}{3} \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "1 & \\frac{2}{3} & \\frac{2}{3}\n",
    "\\end{bmatrix} - \\begin{bmatrix}\n",
    "0 & -\\frac{2}{3} & -\\frac{4}{15} \\\\\n",
    "0 & \\frac{1}{3} & \\frac{2}{15} \\\\\n",
    "0 & 1 & \\frac{2}{5} \\\\\n",
    "0 & \\frac{1}{3} & \\frac{2}{15}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 & 0 & \\frac{9}{15} \\\\\n",
    "0 & 0 & \\frac{3}{15} \\\\\n",
    "0 & 0 & \\frac{3}{5} \\\\\n",
    "0 & 0 & -\\frac{12}{15}\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Можно заметить, что в действительности $a_k$ столбец матрицы $A$ либо первый столбец матрицы $A^{(k)}$ произведение $a_k = q_kr_{kk}$, где $q_k^{\\top}q_k = 1$, и:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "&a_k =  q_k r_{kk} \\\\\n",
    "&q_k = \\frac{a_k}{r_{kk}} \\\\\n",
    "&r_{kk} = \\|a_k\\|_2      \n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Еще одно короткое наблюдение, если мы знаем вектор $q_k^{\\top}$, то нам не составит труда посчитать $r_k$, так как колонки матрицы $Q$ ортогональны друг другу и единственное ненулевое произведение, когда $i = k$:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "       q_k^T \\begin{bmatrix} 0 & A^{(k)} \\end{bmatrix} = q_k^T \\Bigg( \\sum\\limits_{i=k}^n q_i r_i^T \\Bigg) = r_k^T\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Таким образом, чтобы получить первую колонку матрицы $Q$ и первую строчку матрицы $R$, делаем слелующую процедуру:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "A &= \\sum\\limits_{i=1}^n q_i r_i^T \\\\\n",
    "&= q_1r_1^T + \\cdots + q_n r_n^T \\\\\n",
    "Ae_1 &= \\Big( q_1 r_1^T + \\cdots + q_n r_n^T \\Big) e_1 & \\mbox{первая колонка с обоих сторон} \\\\\n",
    "Ae_1 &= q_1 r_{11} \\\\\n",
    "a_1 &= Ae_1 = q_1 r_{11} \\\\\n",
    "r_{11} &= \\|a_1 \\|_2 \\\\\n",
    "q_1 &= a_1 / r_{11} \\\\\n",
    "q_1^T A &= q_1^T (q_1 r_1^T + \\cdots + q_n r_n^T) = r_1^T \\\\\n",
    "A - q_1 r_1^T &= \\sum\\limits_{i=2}^n q_i r_i^T\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "И далее по индукции общая формула для $QR$ разложения уже была получена выше:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "       a_1 = Ae_1 = \\sum\\limits_{i=1}^n q_i r_i^T e_1 = q_1 r_{11}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "   q_1^T A = q_1^T ( \\sum\\limits_{i=1}^n q_i r_i^T) = r_1^T\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    " \\begin{bmatrix} 0 & A^{(2)} \\end{bmatrix} = A - q_1 r_1^T = \\sum\\limits_{i=2}^n q_i r_i^T\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Теперь можно провести весь процесс на матрице, предложенной выше:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "&A = \\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "1 & 1 & 0\n",
    "\\end{bmatrix} \\\\\n",
    "&q_1 = \\frac{a_1}{\\|a_1\\|_2} = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ 0 \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix},\\,  r_1^{\\top} = q_1^{\\top}A = \\begin{pmatrix} \\frac{3}{\\sqrt{3}} & \\frac{2}{\\sqrt{3}} & \\frac{2}{\\sqrt{3}} \\end{pmatrix}, \\, A^{(2)} = A - q_1 r_1^T = \n",
    "\\begin{bmatrix}\n",
    "0 & -\\frac{2}{3} & \\frac{1}{3} \\\\\n",
    "0 & \\frac{1}{3} & \\frac{1}{3} \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "0 & \\frac{1}{3} & -\\frac{2}{3}\n",
    "\\end{bmatrix} \\\\\n",
    "&q_2 = \\frac{\\begin{pmatrix} -\\frac{2}{3} \\\\ \\frac{1}{3} \\\\ 1 \\\\ \\frac{1}{3} \\end{pmatrix}}{\\frac{\\sqrt{15}}{3}} = \\begin{pmatrix} -\\frac{2}{\\sqrt{15}} \\\\ \\frac{1}{\\sqrt{15}} \\\\ \\frac{3}{\\sqrt{15}} \\\\ \\frac{1}{\\sqrt{15}} \\end{pmatrix}, \\, r_2^{\\top} = q_2^{\\top}A^{(2)} = \\begin{pmatrix} 0 & \\frac{5}{\\sqrt{15}} & \\frac{2}{\\sqrt{15}} \\end{pmatrix}, \\, A^{(3)} = A^{(2)} - q_2 r_2^T = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 & \\frac{3}{5} \\\\\n",
    "0 & 0 & \\frac{1}{5} \\\\\n",
    "0 & 0 & \\frac{3}{5} \\\\\n",
    "0 & 0 & -\\frac{4}{5}\n",
    "\\end{bmatrix}\\\\ \n",
    "&q_3 = \\begin{pmatrix} \\frac{3}{\\sqrt{35}} \\\\ \\frac{1}{\\sqrt{35}} \\\\ \\frac{3}{\\sqrt{35}} \\\\ -\\frac{4}{\\sqrt{35}}\\end{pmatrix}, \\, r_3^{\\top} = q_3^{\\top}A^{(3)} = \\begin{pmatrix} 0 & 0 & \\frac{\\sqrt{35}}{5} \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57c9ef",
   "metadata": {},
   "source": [
    "Kлючевая геометрическая концепция, лежащая в основе процесса Грама-Шмидта - это проецирование векторов. Для каждого вектора мы вычитаем компоненты, которые указывают в направлении предыдущих векторов. Это гарантирует, что новый вектор будет ортогонален предыдущим. Удаляя эти проекции, мы получаем новые векторы, находящиеся перпендикулярно по отношению к другим. Каждый шаг процесса Грама-Шмидта добавляет одно новое ортогональное направление. По мере прохождения процесса строится ортогональный базис для подпространства, охватываемого исходными векторами."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc4766",
   "metadata": {},
   "source": [
    "### Хаусхольдер QR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc2b7ea",
   "metadata": {},
   "source": [
    "Процедура ортогонализации Грама-Шмидта обычно не рекомендуется для численного использования. Предположим, мы записываем $A = [a_1 \\dots a_m]$ и $Q = [q_1 \\dots q_m]$. Основная проблема заключается в том, что если $r_{jj} \\leq\\leq \\|a_j \\|_2$, то на этапе вычитания может разрушиться точность вычисленного $q_j$; и в частности, вычисленный $q_j$ может быть не совсем ортогонален предыдущему $q_j$. На самом деле, потеря ортогональности может накапливаться, даже если диагональные элементы $R$ не являются исключительно малыми. Это нехорошо, и хотя существуют приемы для решения этой проблемы, необходим другой подход, если мы хотим, чтобы проблема полностью исчезла.\n",
    "\n",
    "Например, возьмем округление до 7 знаков:\n",
    "\n",
    "$\n",
    "a_1 = \\begin{pmatrix}1 \\\\ 1 \\end{pmatrix}, \\, a_2 = \\begin{pmatrix}1 \\\\ 1.0000001 \\end{pmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "&q_1 = \\frac{a_1}{\\|a_1\\|_2} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix}0.7071067 \\\\ 0.7071067 \\end{pmatrix} \\\\\n",
    "&\\text{proj}_{q_1}(a_2) = \\frac{<a_2,q_1>}{<q_1,q_1>}q_1 = \\frac{1*0.7071067+1.0000001*0.7071067}{1}\\begin{pmatrix}0.7071067 \\\\ 0.7071067 \\end{pmatrix} = \\begin{pmatrix}0.9999998 \\\\ 0.9999998 \\end{pmatrix}\\\\\n",
    "&u_2 = a_2 - \\text{proj}_{q_1}(a_2) = \\begin{pmatrix}1 \\\\ 1.0000001 \\end{pmatrix} - \\begin{pmatrix}0.9999998 \\\\ 0.9999998 \\end{pmatrix} = \\begin{pmatrix}0.0000002 \\\\ 0.0000003\\end{pmatrix} \\\\\n",
    "&q_2 = \\frac{u_2}{\\|u\\|_2} = \\frac{1}{0.0000004}\\begin{pmatrix}0.0000002 \\\\ 0.0000003\\end{pmatrix} = \\begin{pmatrix}0.5\\\\ 0.75\\end{pmatrix} \\\\\n",
    "&<q_1,q_2> = 0.7071067*0.5 + 0.7071067*0.75 = 0.88\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Что должно быть равно нулю."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8409e5",
   "metadata": {},
   "source": [
    "### Преобразование Хаусхольдера "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ba3e7",
   "metadata": {},
   "source": [
    "Напомним, что один из способов реализации алгоритма Гаусса — это LU разложение, которое используется для сведения матрицы к нижнетреугольной. Преобразования Хаусхолдера — это ортогональные преобразования (отражения), которые можно использовать для аналогичного эффекта. Отражение поперек плоскости, ортогональной единичному нормальному вектору $w$, можно выразить в матричной форме как:\n",
    "\n",
    "$$\n",
    "w^{\\top}w = 1, \\quad P = \\mathbf{I} - 2ww^{\\top} - \\text{матрица Хаусхольдера}\n",
    "$$\n",
    "\n",
    "Что означает данная матрица геометрически, перепишем с нормировкой, как было разобрано ранее про внешнее произведение: $P = \\mathbf{I} - 2\\frac{ww^{\\top}}{w^{\\top}w}$, также вспомним, что $\\overset{\\rightharpoonup}{\\text{proj}}_w(x) = \\frac{ww^{\\top}}{w^{\\top}w}x$, являеся веткорной проекцией вектра $x$ на вектор $w$, **векторная проекция** это вектор исходящий из начала координат (начала векторов) длиной проекции вектора $x$ на вектор $w$ по направлению вектора $w$. Очевидно, что векторная проекция может быть отрицательной при отрицательной скалярной проекции, такое происходит, когда вектора направлены в разные стороны, когда скалярное произведение векторов отрицательно, так как знак скалярного произведения это косинус угла между векторами, а косинус положителен когда угол между векторами лежит в диапазоне $[-\\frac{\\pi}{2};\\frac{\\pi}{2}]$, если угол между векторами в абсолютном значении лежит в диапазоне $[\\frac{\\pi}{2};\\pi]$, то скалярное произведение в данном случае будет отрицательно. Также напомним, что является отражением, отражение это задается через отражение по оси $y$, если $X \\in \\mathbb{R}^2, (x,y) \\rightarrow_{\\text{поворот}} (x, -y)$. Если нас инетересует угол поворота при отражении, то нам необходимо посчитать $\\cos(\\theta) = \\frac{x^2 - y^2}{x^2 + y^2}$, что вообще может напоминать следующую формулу при геометрической подстановке: $x = r\\cos\\phi, y = r\\sin\\phi \\rightarrow \\cos(\\theta) = \\frac{r^2 (\\cos^2\\phi - \\sin^2\\phi)}{r^2} = \\cos(2\\phi) \\rightarrow \\theta = 2\\phi$, угол отражения равен двойному углу поворота в сферической системе координат, что идентично и для декартовых координат, следовательно угол отражения равен удвоенному углу поворота до оси $y$. В общем, можно рассмотреть следующий простой пример, чтобы убедиться, что эа матрица действительно является матрицей отражения, отразим $(x, y)$ через плоскость $y$, которую можно задать единичным вектором $e_2 = (0,1), 2\\frac{ww^{\\top}}{w^{\\top}w} = 2\\frac{e_2e_2^{\\top}}{e_2^{\\top}e_2} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 2 \\end{pmatrix}$, посчитаем $P(x,y) = (x,y)^{\\top} - 2\\frac{ww^{\\top}}{w^{\\top}w}(x,y)^{\\top} = \\begin{pmatrix}x \\\\y \\end{pmatrix} - \\begin{pmatrix}0 \\\\2y \\end{pmatrix} = \\begin{pmatrix}x \\\\-y \\end{pmatrix}$. \n",
    "\n",
    "Разумеется, что отразить мы могли относительно любого вектора, смысл в том, что сама процедура отражения происходит в два этапа: нахождение векторной проекции вектора на ось, относительно которой мы будем отражать и вычитание из первоначального вектора данное отражение дважды, вычитая дважды проекцию, мы сначала удаляем компоненту $x$ в направлении $w$, а затем добавляем ее обратно в противоположном направлении, отражая вектор относительно заданной оси $w$.\n",
    "\n",
    "Для данной матрицы есть несколько полезных свойств, например **симметричность и ортогональность**:\n",
    "\n",
    "**Доказательство симметричности:**\n",
    "\n",
    "$\n",
    "P^{\\top} = (\\mathbf{I} - 2ww^{\\top})^{\\top} = \\mathbf{I} - 2(w^{\\top})^{\\top}w^{\\top} = \\mathbf{I} - 2ww^{\\top}\n",
    "$\n",
    "\n",
    "**Доказательство ортогональности:**\n",
    "\n",
    "$\n",
    "PP^{\\top} = (\\mathbf{I} - 2ww^{\\top})(\\mathbf{I} - 2ww^{\\top}) = \\mathbf{I} - 4ww^{\\top} + 4ww^{\\top}ww^{\\top} = \\mathbf{I} - 4ww^{\\top} + 4w(w^{\\top}w)w^{\\top} = \\mathbf{I} - 4ww^{\\top} + 4ww^{\\top} = \\mathbf{I}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa59170",
   "metadata": {},
   "source": [
    "#### Хаусхольдер - Гаусс?\n",
    "\n",
    "Матрица Хаусхолдера (или отражения Хаусхольдера) похожи на преобразования Гаусса в том, что они могут использоваться для обнуления выбранных компонентов вектора. В частности, предположим, что нам дан $x \\in \\mathbb{R}^{m}$ и мы хотим найти такую матрицу $P$:\n",
    "\n",
    "$\n",
    "Px = \n",
    "\\begin{pmatrix}\n",
    "||x||_2 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "Отнормируем матрицу Хаусхольдера и перепишем ее в общем виде без добавления ограничения на его норму $P = \\mathbf{I} - 2\\frac{ww^{\\top}}{w^{\\top}w}$, вектор $w$ выбирается таким образом, что при применении преобразования Хаусхолдера к $x$ получается вектор, который был бы лишь кратным стандартному базисному вектору $e_1 = (1, 0, \\dots, 0)^{\\top}$. Мы назначаем такой вектор $w = x + \\alpha e_1$ для некоторого скаляра $\\alpha$, где $e_1$ — первый стандартный базисный вектор в $\\mathbb{R}^m$. Логика выбора такого вектора состоит в том, чтобы занулить все координаты $x$ кроме координаты указанной кратности, так как векторное отображение $x$ на $x$ это в точности $x$ с амлитудой 1. Выбор $\\alpha$ имеет решающее значение для обеспечения того, чтобы преобразованный вектор $Px$ имел нули в желаемых позициях. Для достижения нашей задачи мы накладываем такое условие, что $Px$ должно быть кратно $e_1$. Это приводит нас к решению для $\\alpha$, при котором коэффициент при $x$ в преобразованном векторе становится равным нулю.\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "&w^{\\top}x = x^{\\top}x + \\alpha x_1 \\\\\n",
    "&w^{\\top}w = x^{\\top}x + 2\\alpha x_1 + \\alpha^2 \\\\\n",
    "&Px = x - 2\\frac{w^{\\top}x}{w^{\\top}w}w \\\\\n",
    "&Px = x - 2\\frac{(x + \\alpha e_1)(x^{\\top}x + \\alpha x_1 )}{x^{\\top}x + 2\\alpha x_1 + \\alpha^2} \\\\\n",
    "&Px = (1 - \\frac{x^{\\top}x + \\alpha x_1}{x^{\\top}x + 2\\alpha x_1 + \\alpha^2})x - 2\\alpha\\frac{w^{\\top}x}{w^{\\top}w}e_1 \\\\\n",
    "&Px = (\\frac{\\alpha^2 - ||x||_2^2}{x^{\\top}x + 2\\alpha x_1 + \\alpha^2})x - 2\\alpha\\frac{w^{\\top}x}{w^{\\top}w}e_1\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Чтобы коэффициент при $x$ был нулевой выбираем $\\alpha = \\pm ||x||_2$, тогда:\n",
    "\n",
    "$\n",
    "w = x \\pm ||x||_2 e_1 \\rightarrow Px = (\\mathbf{I} - 2\\frac{ww^{\\top}}{w^{\\top}w})x = ||x||_2e_1\n",
    "$\n",
    "\n",
    "Таким образом последовательно примененяя операцию отражения на вектор в матрице, мы можем подобрать такую ось отражения, что данная процедура будет в точности копировать результат алгоритма Гаусса.\n",
    "\n",
    "Например:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "&\\begin{bmatrix}\n",
    "3 & 1 \\\\\n",
    "4 & 2\n",
    "\\end{bmatrix} \\\\\n",
    "&w = \\begin{pmatrix} 3 \\\\ 4\\end{pmatrix} - \\sqrt{3^2+4^2}\\begin{pmatrix} 1 \\\\0 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 4\\end{pmatrix} \\\\\n",
    "&2(\\frac{ww^{\\top}}{w^{\\top}w})x = \\frac{2}{20}\\begin{bmatrix}\n",
    "4 & -8 \\\\\n",
    "-8 & 16\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "3 \\\\\n",
    "4 \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "-2 \\\\\n",
    "4\n",
    "\\end{bmatrix} \\\\\n",
    "&Px = \\begin{pmatrix} 5 \\\\ 0 \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "В частности, мы можем отображать не только на один единичный вектор, но и на совокупность векторов $y$, если $x,y: ||x|| = ||y||$, тогда $w = x - y$, здесь переход $e = \\frac{y}{\\|y\\|} = \\frac{y}{\\|x\\|} \\rightarrow w = x - \\|x\\|e = x - y$\n",
    "\n",
    "Таким образом, если нам дан вектор $x$, и мы хотим найти отражение, которое преобразует $x$ в направление, параллельное некоторому единичному вектору $y$. Правильное отражение проходит через гиперплоскость, которая является биссектрисой угла между $x$ и $y$.\n",
    "\n",
    "И некоторые другие свойства касательно матрицы Хаусхольдера:\n",
    "\n",
    "a) $Pw = -w$ и для любого $x\\perp w, \\, w^{\\top}x = 0 \\rightarrow Px = x$ говорит нам о том, что матрица $P$ имеет собственное значение $\\lambda_1 = 1$ кратности $(n-1)$ и $\\lambda_{n} = -1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07046534",
   "metadata": {},
   "source": [
    "**Пример**\n",
    "\n",
    "$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "4 & 4 & 2\\\\\n",
    "4 & 5 & 3\\\\\n",
    "2 & 3 & 3\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Для матрицы $A$ мы хотим превратить $x_1 = (4,4,2) \\rightarrow y_1 = (*,0,0)$, найдем этот вектор, $||x_1|| = 36 \\rightarrow y_1 = (6,0,0)$\n",
    "\n",
    "$$\n",
    "w = \\frac{\n",
    "\\begin{pmatrix} 4\\\\ 4 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 6\\\\ 0 \\\\ 0 \\end{pmatrix}}{||\\begin{pmatrix} 4\\\\ 4 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 6\\\\ 0 \\\\ 0 \\end{pmatrix}||} = \\begin{pmatrix} \\frac{-1}{6}\\\\ \\frac{2}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{6}} \\end{pmatrix}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "w^{\\top}w = \\begin{bmatrix}\n",
    "\\frac{1}{6} & \\frac{-2}{6} & \\frac{-1}{6}\\\\\n",
    "\\frac{-2}{6} & \\frac{4}{6} & \\frac{2}{6}\\\\\n",
    "\\frac{-1}{6} & \\frac{2}{6} & \\frac{1}{6}\n",
    "\\end{bmatrix}\\\\\n",
    "\\newline\n",
    "P = \\mathbf{I} - 2w^{\\top}w  = \n",
    "\\begin{bmatrix}\n",
    "\\frac{4}{6} & \\frac{4}{6} & \\frac{2}{6}\\\\\n",
    "\\frac{4}{6} & - \\frac{2}{6} & -\\frac{4}{6}\\\\\n",
    "\\frac{2}{6} & -\\frac{4}{6} & \\frac{4}{6}\n",
    "\\end{bmatrix}\\\\\n",
    "P\\begin{pmatrix} 4\\\\ 4 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 6\\\\ 0 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Ну а для получения матрицы данного преобразования:\n",
    "\n",
    "$$\n",
    "PA = \\begin{bmatrix}\n",
    "6 & 7 & \\frac{13}{3}\\\\\n",
    "0 & -1 & -\\frac{5}{3}\\\\\n",
    "0 & 0 & \\frac{2}{3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Аналогичные преобразования можно получать и для строчек (минусая столбцы) умножая на матрицу Хаусхольдера справа $AP$, аналогично сводя матрицу к треугольному виду\n",
    "\n",
    "Таким образом имеем $QR$ разложение для матрицы $A$:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "4 & 4 & 2\\\\\n",
    "4 & 5 & 3\\\\\n",
    "2 & 3 & 3\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{4}{6} & \\frac{4}{6} & \\frac{2}{6}\\\\\n",
    "\\frac{4}{6} & - \\frac{2}{6} & -\\frac{4}{6}\\\\\n",
    "\\frac{2}{6} & -\\frac{4}{6} & \\frac{4}{6}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "6 & 7 & \\frac{13}{3}\\\\\n",
    "0 & -1 & -\\frac{5}{3}\\\\\n",
    "0 & 0 & \\frac{2}{3}\n",
    "\\end{bmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c365dc",
   "metadata": {},
   "source": [
    "### Вращениe Гивенса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869faf5d",
   "metadata": {},
   "source": [
    "Другим стандартным ортогональным преобразованием является вращение Гивенса.\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "&G = \n",
    "\\begin{bmatrix}\n",
    "c & \\mp s \\\\\n",
    "\\pm s & c\n",
    "\\end{bmatrix}, \\, det(G) = s^2 + c^2 =  1 \\\\\n",
    "&G\\begin{pmatrix} x \\\\y \\end{pmatrix} = \\begin{pmatrix} cx \\mp sy \\\\ \\pm sx + cy \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Выбирая, $s = \\frac{\\mp y}{\\sqrt{x^2+y^2}}, c = \\frac{\\pm x}{\\sqrt{x^2+y^2}}$, можно гарантировать ноль во вторрой коолонке. \n",
    "\n",
    "В более общем случае мы можем преобразовать вектор в $\\mathbb{R}^{m}$ в вектор, параллельный $e_1$, последовательностью из $m - 1$ вращений Гивенса, где первое вращение обращает последний элемент в ноль, второе - предпоследний элемент в ноль и так далее. Для некоторых приложений требуется введение нулей по одному элементу. В некоторых местах можно видеть, что это основное значимое различие между алгоритмами Хаусхольдера и Гивенса (помимо вычислительной сложности). Стоит подчеркнуть, что можно модифицировать алгоритм отражений Хаусхольдера так, чтобы при использовании введился только один ноль за раз. Тем не менее, в общем случае вращение Гивенса кажется более популярным выбором для такого рода локального введения нулей.\n",
    "\n",
    "Ранее мы говорили про то, что отражение Хаусхольдера в случае $X \\in \\mathbb{R}^2$ это поворот на некоторый угол $\\cos(\\theta) = \\frac{x^2 - y^2}{x^2 + y^2}$, в котором достигается $X \\in \\mathbb{R}^2, (x,y) \\rightarrow_{\\text{поворот}} (x, -y)$, угол теперь можно переписать в следующих терминах для отображения $\\cos(\\theta) = с^2 - s^2$ матрицы $G$, либо $\\cos(\\theta) = sin^2\\alpha - cos^2\\alpha = \\cos(2\\alpha)$, если вспомнить часть Хаусхольдера, либо часть курса школьной геометрии. Таким образом, матрица $G$ представима ввиде:\n",
    "\n",
    "$\n",
    "G^{\\text{rotation}} = \n",
    "\\begin{bmatrix}\n",
    "\\cos\\alpha & \\mp \\sin\\alpha \\\\\n",
    "\\pm \\sin\\alpha & \\cos\\alpha\n",
    "\\end{bmatrix}, \\, \\text{где}\\, \\cos\\alpha = \\frac{\\pm x}{\\sqrt{x^2+y^2}}, \\sin\\alpha = \\frac{\\mp y}{\\sqrt{x^2+y^2}}\n",
    "$\n",
    "\n",
    "Данная матрица называется матрицей **поворота Гивенса**. Знак определяется углом поворота: если угол поворота положителен - то верхний знак, отрицательный - наоборот. По ранее сказанному, можно также вывести **матрицу отражения**\n",
    "\n",
    "$\n",
    "G^{\\text{reflection}} = \n",
    "\\begin{bmatrix}\n",
    "\\cos2\\alpha & \\sin2\\alpha \\\\\n",
    "\\sin2\\alpha & -\\cos2\\alpha\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "**Не путать** c матрицей отражения Хаусхольдера, так как матрица отражения относительно оси $x$ является частным случаем матрицы Хаусхольдера. \n",
    "\n",
    "Задача Найти такой $w$ для матрицы отражения Хаусхольдера, которое воссоздает классическое отражение $(x,y)\\rightarrow (x,-y)$.\n",
    "\n",
    "Решение:\n",
    "\n",
    "В преобразованиях Хаусхольдера вектор $w$ задает направление, перпендикулярное гиперплоскости (или прямой в 2D), через которую происходит отражение. Чтобы отразиться от прямой под углом $\\alpha$, $w$ должен быть нормалью к линии, тое есть перпендикулярным линии.\n",
    "\n",
    "**Найдем уравнение и нормаль к прямой, которую собираемся отражать:**\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "&y = \\tan(\\alpha)x \\rightarrow \\tan(\\alpha)x - 1*y = 0\\\\\n",
    "&d = \\begin{pmatrix} \\tan(\\alpha) \\\\ -1 \\end{pmatrix} \\\\\n",
    "&\\|d\\| = \\sqrt{1 +\\tan^2(\\alpha)} = \\frac{1}{|\\cos(\\alpha)|}\\\\\n",
    "&d_{\\text{unit}} = \\frac{d}{\\|d\\|} = \\begin{pmatrix} \\sin(\\alpha) \\\\ -\\cos(\\alpha) \\end{pmatrix} \n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "**Нормаль найдена**\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "&w = \\begin{pmatrix} \\sin(\\alpha) \\\\ -\\cos(\\alpha) \\end{pmatrix} \\\\\n",
    "&P = \\mathbf{I} - 2\\frac{ww^{\\top}}{w^{\\top}w} = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix} - \n",
    "2\\begin{bmatrix}\n",
    "\\sin^2\\alpha & -\\sin\\alpha\\cos\\alpha \\\\\n",
    "-\\sin\\alpha\\cos\\alpha & \\cos^2\\alpha\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1-\\sin^2\\alpha & 2\\sin\\alpha\\cos\\alpha \\\\\n",
    "2\\sin\\alpha\\cos\\alpha & 1-\\cos^2\\alpha\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\cos2\\alpha & \\sin2\\alpha \\\\\n",
    "\\sin2\\alpha & -\\cos2\\alpha\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66153e4",
   "metadata": {},
   "source": [
    "### Вычислительные сложности"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa036a",
   "metadata": {},
   "source": [
    "Для матрицы $A \\in \\mathbb{R}^{m\\times n}$\n",
    "\n",
    "**Грам-Шмидт**\n",
    "\n",
    "Скалярное произведение и нормализация:\n",
    "\n",
    "* Для каждого из $n$ столбцов $A$ необходимо вычислить скалярные произведения с предыдущими ортогональными векторами (не более $n-1$ скалярных произведений на столбец). Каждое скалярное произведение - $O(m)$ операций.\n",
    "* Нормализация каждого вектора (одно деление и $m$ умножений) -  $O(m)$ на столбец.\n",
    "* Общая сложность для $n$ столбцов: $O(mn^2)$.\n",
    "\n",
    "Ортогонализация:\n",
    "* Для каждого столбца вычесть проекции на предыдущие ортогональные векторы (не более $n-1$ вычитаний на столбец, каждое стоимостью $O(m)$).\n",
    "* Общая сложность для $n$ столбцов: $O(mn^2)$.\n",
    "\n",
    "Итого: $2mn^2$\n",
    "\n",
    "**Хаусхольдер**\n",
    "\n",
    "$\n",
    "A_{k:m,k:n} =  A_{k:m,k:n} - 2v_k (v_{k}^TA_{k:m,k:n})\n",
    "$\n",
    "\n",
    "* Внешнее произведние (проекция) - $(m-k)(n-k) - (2v_k (*))$\n",
    "* Скалярное произведение - $2(m-k)(n-k) - (v_{k}^TA_{k:m,k:n})$\n",
    "* Вычитание - $(m-k)(n-k) - (A_{k:m,k:n} - *)$\n",
    "\n",
    "Итого: $\\sum_{k=1}^n4(m-k)(n-k) \\approx 2mn^2 - \\frac{2}{3}n^3$\n",
    "\n",
    "**Гивенс**\n",
    "\n",
    "Нам нужно занулить все элементы под главной диагональю $\\frac{nm - n^2}{2}$ для матрицы вращения $2\\times 2$ нужно произвести изменение для двух строчек 2 умножения и 1 вычитание и сложения, что дает $6n$ операций для зануления одного элемента, таким образом $6n\\frac{nm - n^2}{2}$\n",
    "\n",
    "Итого: $3mn^2 - n^3$\n",
    "\n",
    "**Нормальное уравнение**\n",
    "\n",
    "Умножение $A^{\\top}A, A\\in\\mathbb{R}^{m\\times n}$ стоит приблизительно $mn^2$ операций, плюс разложение Холецкого, что тоже самое, что и $LU$ для симметричных матриц $\\frac{1}{3}n^3$\n",
    "\n",
    "Итого: $mn^2 + \\frac{1}{3}n^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d4685a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import lu\n",
    "A = np.array([[2, 5], [5, 2], [7, 5], [5, 4]])\n",
    "b = np.array([1, -1, -2, 2])\n",
    "p, l, u = lu(A)\n",
    "Q, R = np.linalg.qr(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "165d381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_solve():\n",
    "    \n",
    "    tilde_b = Q.T @ b\n",
    "    solution = np.linalg.solve(R, tilde_b)\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "270ead35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lu_solve():\n",
    "    \n",
    "    tilde_b = p.T @ b\n",
    "    _, m_slice = A.shape\n",
    "    y_1 = np.linalg.solve(l[:m_slice,:], tilde_b[:m_slice])\n",
    "    y_2 = np.linalg.solve(u, y_1)\n",
    "    return y_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e7475394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_solve():\n",
    "    C = A.T@A\n",
    "    d = A.T@b\n",
    "    G = np.linalg.cholesky(C)\n",
    "    y_1 = np.linalg.solve(G, d)\n",
    "    solution = np.linalg.solve(G.T, y_1)\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fb920553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on qr/normal: 2.666\n",
      "Error on lu: 3.428\n"
     ]
    }
   ],
   "source": [
    "qr_x = qr_solve()\n",
    "lu_x = lu_solve()\n",
    "normal_x = normal_solve()\n",
    "print(f'Error on qr/normal: {np.linalg.norm(A@qr_x-b):.3f}\\nError on lu: {np.linalg.norm(A@lu_x-b):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e579d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
