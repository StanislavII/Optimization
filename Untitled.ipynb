{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc3075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Optimizer\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "bias = 3\n",
    "weight = 1\n",
    "X = torch.arange(start=0, end=4, step=0.02).reshape(-1, 1)\n",
    "noise = torch.randint_like(X, low=-100, high=100) / 200 * X\n",
    "y = weight * X + bias + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96cf7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(3), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.randn(3),requires_grad=True)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.weights * x + self.bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38dbd166",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03f54242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW as AdamW_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac25a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/transformers/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = AdamW_(params=model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a6c635ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class AdamW(Optimizer):\n",
      "    \"\"\"\n",
      "    Implements Adam algorithm with weight decay fix as introduced in [Decoupled Weight Decay\n",
      "    Regularization](https://arxiv.org/abs/1711.05101).\n",
      "\n",
      "    Parameters:\n",
      "        params (`Iterable[nn.parameter.Parameter]`):\n",
      "            Iterable of parameters to optimize or dictionaries defining parameter groups.\n",
      "        lr (`float`, *optional*, defaults to 1e-3):\n",
      "            The learning rate to use.\n",
      "        betas (`Tuple[float,float]`, *optional*, defaults to (0.9, 0.999)):\n",
      "            Adam's betas parameters (b1, b2).\n",
      "        eps (`float`, *optional*, defaults to 1e-6):\n",
      "            Adam's epsilon for numerical stability.\n",
      "        weight_decay (`float`, *optional*, defaults to 0):\n",
      "            Decoupled weight decay to apply.\n",
      "        correct_bias (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not to correct bias in Adam (for instance, in Bert TF repository they use `False`).\n",
      "        no_deprecation_warning (`bool`, *optional*, defaults to `False`):\n",
      "            A flag used to disable the deprecation warning (set to `True` to disable the warning).\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        params: Iterable[nn.parameter.Parameter],\n",
      "        lr: float = 1e-3,\n",
      "        betas: Tuple[float, float] = (0.9, 0.999),\n",
      "        eps: float = 1e-6,\n",
      "        weight_decay: float = 0.0,\n",
      "        correct_bias: bool = True,\n",
      "        no_deprecation_warning: bool = False,\n",
      "    ):\n",
      "        if not no_deprecation_warning:\n",
      "            warnings.warn(\n",
      "                \"This implementation of AdamW is deprecated and will be removed in a future version. Use the\"\n",
      "                \" PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\",\n",
      "                FutureWarning,\n",
      "            )\n",
      "        require_version(\"torch>=1.5.0\")  # add_ with alpha\n",
      "        if lr < 0.0:\n",
      "            raise ValueError(f\"Invalid learning rate: {lr} - should be >= 0.0\")\n",
      "        if not 0.0 <= betas[0] < 1.0:\n",
      "            raise ValueError(f\"Invalid beta parameter: {betas[0]} - should be in [0.0, 1.0)\")\n",
      "        if not 0.0 <= betas[1] < 1.0:\n",
      "            raise ValueError(f\"Invalid beta parameter: {betas[1]} - should be in [0.0, 1.0)\")\n",
      "        if not 0.0 <= eps:\n",
      "            raise ValueError(f\"Invalid epsilon value: {eps} - should be >= 0.0\")\n",
      "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)\n",
      "        super().__init__(params, defaults)\n",
      "\n",
      "    def step(self, closure: Callable = None):\n",
      "        \"\"\"\n",
      "        Performs a single optimization step.\n",
      "\n",
      "        Arguments:\n",
      "            closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.\n",
      "        \"\"\"\n",
      "        loss = None\n",
      "        if closure is not None:\n",
      "            loss = closure()\n",
      "\n",
      "        for group in self.param_groups:\n",
      "            for p in group[\"params\"]:\n",
      "                if p.grad is None:\n",
      "                    continue\n",
      "                grad = p.grad.data\n",
      "                if grad.is_sparse:\n",
      "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
      "\n",
      "                state = self.state[p]\n",
      "\n",
      "                # State initialization\n",
      "                if len(state) == 0:\n",
      "                    state[\"step\"] = 0\n",
      "                    # Exponential moving average of gradient values\n",
      "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
      "                    # Exponential moving average of squared gradient values\n",
      "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
      "\n",
      "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
      "                beta1, beta2 = group[\"betas\"]\n",
      "\n",
      "                state[\"step\"] += 1\n",
      "\n",
      "                # Decay the first and second moment running average coefficient\n",
      "                # In-place operations to update the averages at the same time\n",
      "                exp_avg.mul_(beta1).add_(grad, alpha=(1.0 - beta1))\n",
      "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)\n",
      "                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
      "\n",
      "                step_size = group[\"lr\"]\n",
      "                if group[\"correct_bias\"]:  # No bias correction for Bert\n",
      "                    bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n",
      "                    bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n",
      "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
      "\n",
      "                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
      "\n",
      "                # Just adding the square of the weights to the loss function is *not*\n",
      "                # the correct way of using L2 regularization/weight decay with Adam,\n",
      "                # since that will interact with the m and v parameters in strange ways.\n",
      "                #\n",
      "                # Instead we want to decay the weights in a manner that doesn't interact\n",
      "                # with the m/v parameters. This is equivalent to adding the square\n",
      "                # of the weights to the loss with plain (non-momentum) SGD.\n",
      "                # Add weight decay at the end (fixed version)\n",
      "                if group[\"weight_decay\"] > 0.0:\n",
      "                    p.data.add_(p.data, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))\n",
      "\n",
      "        return loss\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "lines = inspect.getsource(AdamW_)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beed57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constant_schedule(optimizer, last_epoch=-1):\n",
    "    \"\"\" Create a schedule with a constant learning rate.\n",
    "    \"\"\"\n",
    "    return LambdaLR(optimizer, lambda _: 1, last_epoch=last_epoch)\n",
    "\n",
    "\n",
    "\n",
    "def get_constant_schedule_with_warmup(optimizer, num_warmup_steps, last_epoch=-1):\n",
    "    \"\"\" Create a schedule with a constant learning rate preceded by a warmup\n",
    "    period during which the learning rate increases linearly between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1.0, num_warmup_steps))\n",
    "        return 1.0\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "\n",
    "\n",
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n",
    "    \"\"\" Create a schedule with a learning rate that decreases linearly after\n",
    "    linearly increasing during a warmup period.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, last_epoch=-1):\n",
    "    \"\"\" Create a schedule with a learning rate that decreases following the\n",
    "    values of the cosine function between 0 and `pi * cycles` after a warmup\n",
    "    period during which it increases linearly between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "\n",
    "def get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps, num_training_steps, num_cycles=1.0, last_epoch=-1\n",
    "):\n",
    "    \"\"\" Create a schedule with a learning rate that decreases following the\n",
    "    values of the cosine function with several hard restarts, after a warmup\n",
    "    period during which it increases linearly between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        if progress >= 1.0:\n",
    "            return 0.0\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * ((float(num_cycles) * progress) % 1.0))))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "\n",
    "class AdamW1(Optimizer):\n",
    "    \"\"\" Implements Adam algorithm with weight decay fix.\n",
    "\n",
    "    Parameters:\n",
    "        lr (float): learning rate. Default 1e-3.\n",
    "        betas (tuple of 2 floats): Adams beta parameters (b1, b2). Default: (0.9, 0.999)\n",
    "        eps (float): Adams epsilon. Default: 1e-6\n",
    "        weight_decay (float): Weight decay. Default: 0.0\n",
    "        correct_bias (bool): can be set to False to avoid correcting bias in Adam (e.g. like in Bert TF repository). Default True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0, correct_bias=True):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # In-place operations to update the averages at the same time\n",
    "                exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)\n",
    "                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "\n",
    "                step_size = group[\"lr\"]\n",
    "                if group[\"correct_bias\"]:  # No bias correction for Bert\n",
    "                    bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n",
    "                    bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n",
    "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                # Just adding the square of the weights to the loss function is *not*\n",
    "                # the correct way of using L2 regularization/weight decay with Adam,\n",
    "                # since that will interact with the m and v parameters in strange ways.\n",
    "                #\n",
    "                # Instead we want to decay the weights in a manner that doesn't interact\n",
    "                # with the m/v parameters. This is equivalent to adding the square\n",
    "                # of the weights to the loss with plain (non-momentum) SGD.\n",
    "                # Add weight decay at the end (fixed version)\n",
    "                if group[\"weight_decay\"] > 0.0:\n",
    "                    p.data.add_(-group[\"lr\"] * group[\"weight_decay\"], p.data)\n",
    "\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1a99218",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()\n",
    "loss_fn = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "237a3135",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW_(params=model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6592df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params=model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fe8b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "optimizer = AdamW1(params=model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a798dad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9934, grad_fn=<MeanBackward0>)\n",
      "tensor(4.9635, grad_fn=<MeanBackward0>)\n",
      "tensor(4.9336, grad_fn=<MeanBackward0>)\n",
      "tensor(4.9037, grad_fn=<MeanBackward0>)\n",
      "tensor(4.8738, grad_fn=<MeanBackward0>)\n",
      "tensor(4.8439, grad_fn=<MeanBackward0>)\n",
      "tensor(4.8140, grad_fn=<MeanBackward0>)\n",
      "tensor(4.7841, grad_fn=<MeanBackward0>)\n",
      "tensor(4.7542, grad_fn=<MeanBackward0>)\n",
      "tensor(4.7243, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "526597ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6944, grad_fn=<MeanBackward0>)\n",
      "tensor(4.6645, grad_fn=<MeanBackward0>)\n",
      "tensor(4.6346, grad_fn=<MeanBackward0>)\n",
      "tensor(4.6048, grad_fn=<MeanBackward0>)\n",
      "tensor(4.5750, grad_fn=<MeanBackward0>)\n",
      "tensor(4.5453, grad_fn=<MeanBackward0>)\n",
      "tensor(4.5157, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4862, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4566, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4272, grad_fn=<MeanBackward0>)\n",
      "tensor(4.3980, grad_fn=<MeanBackward0>)\n",
      "tensor(4.3689, grad_fn=<MeanBackward0>)\n",
      "tensor(4.3401, grad_fn=<MeanBackward0>)\n",
      "tensor(4.3113, grad_fn=<MeanBackward0>)\n",
      "tensor(4.2827, grad_fn=<MeanBackward0>)\n",
      "tensor(4.2541, grad_fn=<MeanBackward0>)\n",
      "tensor(4.2256, grad_fn=<MeanBackward0>)\n",
      "tensor(4.1971, grad_fn=<MeanBackward0>)\n",
      "tensor(4.1688, grad_fn=<MeanBackward0>)\n",
      "tensor(4.1406, grad_fn=<MeanBackward0>)\n",
      "tensor(4.1128, grad_fn=<MeanBackward0>)\n",
      "tensor(4.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(4.0575, grad_fn=<MeanBackward0>)\n",
      "tensor(4.0301, grad_fn=<MeanBackward0>)\n",
      "tensor(4.0031, grad_fn=<MeanBackward0>)\n",
      "tensor(3.9763, grad_fn=<MeanBackward0>)\n",
      "tensor(3.9495, grad_fn=<MeanBackward0>)\n",
      "tensor(3.9231, grad_fn=<MeanBackward0>)\n",
      "tensor(3.8970, grad_fn=<MeanBackward0>)\n",
      "tensor(3.8711, grad_fn=<MeanBackward0>)\n",
      "tensor(3.8457, grad_fn=<MeanBackward0>)\n",
      "tensor(3.8207, grad_fn=<MeanBackward0>)\n",
      "tensor(3.7959, grad_fn=<MeanBackward0>)\n",
      "tensor(3.7714, grad_fn=<MeanBackward0>)\n",
      "tensor(3.7470, grad_fn=<MeanBackward0>)\n",
      "tensor(3.7229, grad_fn=<MeanBackward0>)\n",
      "tensor(3.6989, grad_fn=<MeanBackward0>)\n",
      "tensor(3.6751, grad_fn=<MeanBackward0>)\n",
      "tensor(3.6517, grad_fn=<MeanBackward0>)\n",
      "tensor(3.6285, grad_fn=<MeanBackward0>)\n",
      "tensor(3.6055, grad_fn=<MeanBackward0>)\n",
      "tensor(3.5829, grad_fn=<MeanBackward0>)\n",
      "tensor(3.5604, grad_fn=<MeanBackward0>)\n",
      "tensor(3.5380, grad_fn=<MeanBackward0>)\n",
      "tensor(3.5156, grad_fn=<MeanBackward0>)\n",
      "tensor(3.4934, grad_fn=<MeanBackward0>)\n",
      "tensor(3.4713, grad_fn=<MeanBackward0>)\n",
      "tensor(3.4493, grad_fn=<MeanBackward0>)\n",
      "tensor(3.4274, grad_fn=<MeanBackward0>)\n",
      "tensor(3.4057, grad_fn=<MeanBackward0>)\n",
      "tensor(3.3841, grad_fn=<MeanBackward0>)\n",
      "tensor(3.3628, grad_fn=<MeanBackward0>)\n",
      "tensor(3.3415, grad_fn=<MeanBackward0>)\n",
      "tensor(3.3203, grad_fn=<MeanBackward0>)\n",
      "tensor(3.2992, grad_fn=<MeanBackward0>)\n",
      "tensor(3.2782, grad_fn=<MeanBackward0>)\n",
      "tensor(3.2573, grad_fn=<MeanBackward0>)\n",
      "tensor(3.2364, grad_fn=<MeanBackward0>)\n",
      "tensor(3.2156, grad_fn=<MeanBackward0>)\n",
      "tensor(3.1948, grad_fn=<MeanBackward0>)\n",
      "tensor(3.1740, grad_fn=<MeanBackward0>)\n",
      "tensor(3.1532, grad_fn=<MeanBackward0>)\n",
      "tensor(3.1326, grad_fn=<MeanBackward0>)\n",
      "tensor(3.1119, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0913, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0707, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0502, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0297, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0092, grad_fn=<MeanBackward0>)\n",
      "tensor(2.9887, grad_fn=<MeanBackward0>)\n",
      "tensor(2.9682, grad_fn=<MeanBackward0>)\n",
      "tensor(2.9477, grad_fn=<MeanBackward0>)\n",
      "tensor(2.9273, grad_fn=<MeanBackward0>)\n",
      "tensor(2.9069, grad_fn=<MeanBackward0>)\n",
      "tensor(2.8865, grad_fn=<MeanBackward0>)\n",
      "tensor(2.8661, grad_fn=<MeanBackward0>)\n",
      "tensor(2.8458, grad_fn=<MeanBackward0>)\n",
      "tensor(2.8256, grad_fn=<MeanBackward0>)\n",
      "tensor(2.8053, grad_fn=<MeanBackward0>)\n",
      "tensor(2.7851, grad_fn=<MeanBackward0>)\n",
      "tensor(2.7649, grad_fn=<MeanBackward0>)\n",
      "tensor(2.7448, grad_fn=<MeanBackward0>)\n",
      "tensor(2.7246, grad_fn=<MeanBackward0>)\n",
      "tensor(2.7045, grad_fn=<MeanBackward0>)\n",
      "tensor(2.6843, grad_fn=<MeanBackward0>)\n",
      "tensor(2.6642, grad_fn=<MeanBackward0>)\n",
      "tensor(2.6440, grad_fn=<MeanBackward0>)\n",
      "tensor(2.6238, grad_fn=<MeanBackward0>)\n",
      "tensor(2.6036, grad_fn=<MeanBackward0>)\n",
      "tensor(2.5835, grad_fn=<MeanBackward0>)\n",
      "tensor(2.5633, grad_fn=<MeanBackward0>)\n",
      "tensor(2.5431, grad_fn=<MeanBackward0>)\n",
      "tensor(2.5229, grad_fn=<MeanBackward0>)\n",
      "tensor(2.5027, grad_fn=<MeanBackward0>)\n",
      "tensor(2.4825, grad_fn=<MeanBackward0>)\n",
      "tensor(2.4623, grad_fn=<MeanBackward0>)\n",
      "tensor(2.4421, grad_fn=<MeanBackward0>)\n",
      "tensor(2.4219, grad_fn=<MeanBackward0>)\n",
      "tensor(2.4016, grad_fn=<MeanBackward0>)\n",
      "tensor(2.3814, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21ea097d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weights', tensor([-1.2039])), ('bias', tensor([-0.4198]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, closure: Callable = None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # In-place operations to update the averages at the same time\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=(1.0 - beta1))\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)\n",
    "                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "\n",
    "                step_size = group[\"lr\"]\n",
    "                if group[\"correct_bias\"]:  # No bias correction for Bert\n",
    "                    bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n",
    "                    bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n",
    "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "                # Just adding the square of the weights to the loss function is *not*\n",
    "                # the correct way of using L2 regularization/weight decay with Adam,\n",
    "                # since that will interact with the m and v parameters in strange ways.\n",
    "                #\n",
    "                # Instead we want to decay the weights in a manner that doesn't interact\n",
    "                # with the m/v parameters. This is equivalent to adding the square\n",
    "                # of the weights to the loss with plain (non-momentum) SGD.\n",
    "                # Add weight decay at the end (fixed version)\n",
    "                if group[\"weight_decay\"] > 0.0:\n",
    "                    p.data.add_(p.data, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d8c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (transformers)",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
